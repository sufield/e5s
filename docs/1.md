You’re right about the root cause: your tests run on the host, but the SPIRE Workload API socket lives **inside the Minikube node**. Fix the *placement* of the tests (or bridge the socket). Below are practical, working options ranked by reliability and speed.

# Option A — Run the tests **inside Kubernetes** (recommended)

Run your integration tests in a pod that mounts the agent socket via `hostPath`. This avoids any socket forwarding games.

### 1) Build a single test binary locally (fast transfer)

```bash
# build just the integration tests into one binary
go test -tags=integration -c \
  -o /tmp/integration.test \
  ./internal/adapters/outbound/spire
```

### 2) Create a minimal runner pod that mounts the socket

> Adjust the `hostPath.path` to match where your agent exposes the socket on the node.
> From your setup that’s `/tmp/spire-agent/public` and socket name `api.sock`.

```bash
cat <<'YAML' | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: spire-it
  namespace: spire-system
spec:
  restartPolicy: Never
  volumes:
    - name: spire-socket
      hostPath:
        path: /tmp/spire-agent/public   # path on the Minikube node
        type: Directory
    - name: work
      emptyDir: {}
  containers:
    - name: runner
      image: gcr.io/distroless/base-debian12  # tiny; we only run a static test binary
      command: ["sleep","infinity"]
      volumeMounts:
        - name: spire-socket
          mountPath: /spire-socket
        - name: work
          mountPath: /work
YAML

kubectl wait -n spire-system --for=condition=Ready pod/spire-it --timeout=60s
```

### 3) Copy and run the test binary inside the pod

```bash
kubectl cp /tmp/integration.test spire-system/spire-it:/work/integration.test
kubectl exec -n spire-system spire-it -- \
  sh -c 'SPIFFE_ENDPOINT_SOCKET=unix:///spire-socket/api.sock /work/integration.test -test.v'
```

### 4) Cleanup

```bash
kubectl delete pod -n spire-system spire-it
```

**Why this is better than `kubectl run golang:1.25` + `go test` inside the pod:**

* You transfer **one small binary**, not your whole repo.
* No module downloads in-cluster.
* Deterministic and quick.

---

# Option B — Package a dedicated test Job (CI-friendly)

When you want zero manual steps, bake the test binary into a small image and run it as a Kubernetes **Job** that mounts the socket. Outline:

1. `go test -c -tags=integration -o integration.test ./...`
2. `Dockerfile` that copies `integration.test` to `/bin/it.test` and sets default `ENTRYPOINT`.
3. `Job` YAML mounting the socket hostPath to `/spire-socket` and running:

   ```
   env:
   - name: SPIFFE_ENDPOINT_SOCKET
     value: unix:///spire-socket/api.sock
   command: ["/bin/it.test","-test.v"]
   ```

This is ideal for CI pipelines.

---

# Option C — Run the **production binary** on the node (quick sanity)

You already sketched this. It’s useful for a smoke test when you don’t need `go test`:

```bash
make prod-build
minikube cp bin/spire-server /tmp/spire-server
minikube ssh 'SPIFFE_ENDPOINT_SOCKET=unix:///tmp/spire-agent/public/api.sock /tmp/spire-server'
```

---

# Option D — Don’t do UDS bridging on the host

Bridging the Unix socket to the host via TCP wrappers (`socat`/tunnels) is fragile and negates the point of the Workload API (which is intentionally UDS). Skip this unless you must.

---

## Pre-flight checks (defensive)

Run these before tests to ensure the socket path you mount is correct:

```bash
# Find the agent DaemonSet and verify how the socket is exposed to pods
kubectl -n spire-system get ds spire-agent -o yaml | grep -A3 -i 'hostPath:\|mountPath:'

# Verify socket on the node (through the agent pod)
AGENT_POD=$(kubectl get pods -n spire-system -l app=spire-agent -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n spire-system "$AGENT_POD" -- test -S /tmp/spire-agent/public/api.sock && echo "socket OK"
```

If your Helm values mount a **different** path to workloads (e.g., `/spiffe-workload-api`), change the `hostPath` in the test pod to match the agent’s **hostPath.source** and set:

```
SPIFFE_ENDPOINT_SOCKET=unix:///spiffe-workload-api/agent.sock
```

---

## Makefile glue (optional)

Add a target that compiles and runs integration tests in-cluster:

```make
.PHONY: itest
itest:
	@echo "Building integration test binary..."
	@go test -tags=integration -c -o /tmp/integration.test ./internal/adapters/outbound/spire
	@echo "Creating runner pod..."
	@kubectl apply -f infra/dev/minikube/spire-it-pod.yaml
	@kubectl wait -n spire-system --for=condition=Ready pod/spire-it --timeout=60s
	@echo "Copying test binary..."
	@kubectl cp /tmp/integration.test spire-system/spire-it:/work/integration.test
	@echo "Running tests..."
	@kubectl exec -n spire-system spire-it -- sh -c 'SPIFFE_ENDPOINT_SOCKET=unix:///spire-socket/api.sock /work/integration.test -test.v'
	@echo "Cleaning up..."
	@kubectl delete pod -n spire-system spire-it || true
```

*(Put the YAML from Option A step 2 at `infra/dev/minikube/spire-it-pod.yaml`.)*

---

## If you prefer to keep tests on the host

Don’t. But if you must:

* **Run SPIRE locally** (server+agent on the host) so `/tmp/spire-agent/public/api.sock` exists on the host. Your current integration tests will pass without changes.
* Or use **Docker Compose** for server+agent and bind-mount the socket to the host; then point `SPIFFE_ENDPOINT_SOCKET` at the host path.

---

* Your adapters are fine; the failure is a **topology** issue (UDS lives inside the node).
* The **clean solution** is to **move the tests to where the socket is**: a pod that mounts the agent socket via `hostPath`.
* Compile tests to a single binary and run it in the pod for **speed and determinism**.
